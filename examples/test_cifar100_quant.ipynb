{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare\n",
    "#### List GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select GPUs to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import package and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MXNet package\n",
    "from mxnet import nd, init, cpu, gpu, gluon, autograd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon.data.vision import CIFAR100, transforms as T\n",
    "from mxnet.gluon.model_zoo.vision import resnet50_v1, mobilenet1_0, mobilenet_v2_1_0\n",
    "\n",
    "# Normal package\n",
    "import time\n",
    "import logging\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Custom package\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils import Timer, ModelConfig, QTrainConfig\n",
    "from quantize.convert import convert_model\n",
    "from quantize.initialize import qparams_init as qinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s]%(name)s: %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mconfig = ModelConfig(\n",
    "    num_class=20\n",
    ")\n",
    "QUANT_OFFLINE_AFTER = 10000\n",
    "MAIN_TAG = \"cifar100_resnet50_v1_quant_relu6\"\n",
    "tconfig = QTrainConfig(\n",
    "    tb_main_tag=MAIN_TAG,\n",
    "    checkpoint_prefix=MAIN_TAG,\n",
    "    param_file=\"./models/cifar100_resnet50-007000.params\",\n",
    "    train_batch_size=16,\n",
    "    val_batch_size=16,\n",
    "    learning_rate=1e-6,\n",
    "    quant_offline_after=QUANT_OFFLINE_AFTER,\n",
    "    spotter_starts_at=QUANT_OFFLINE_AFTER+2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a writer for tensorboard     \n",
    "Named with datetime and main tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_stamp = time.strftime('%Y%m%d_%H%M%S',time.localtime(time.time()))\n",
    "writer = SummaryWriter(log_dir=\"runs/{}_{}\".format(tconfig.tb_main_tag, datetime_stamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the model       \n",
    "1. Define the model       \n",
    "2. Load parameters      \n",
    "3. Convert to quantized version\n",
    "4. Initialize new parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# net = mobilenet1_0(pretrained=False)\n",
    "# net.output = nn.Dense(mconfig.num_class, weight_initializer='normal', bias_initializer='zeros')\n",
    "\n",
    "net = resnet50_v1(pretrained=False)\n",
    "net.output = nn.Dense(mconfig.num_class, weight_initializer='normal', bias_initializer='zeros')\n",
    "\n",
    "net.load_parameters(tconfig.param_file)\n",
    "convert_model(net, exclude=[net.features[0]])  # exclude the first conv\n",
    "qinit(net)\n",
    "_ = net.collect_params().reset_ctx(gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, num_class, dataloader, ctx):\n",
    "    t = time.time()\n",
    "    correct_counter = nd.zeros(num_class)\n",
    "    label_counter = nd.zeros(num_class)\n",
    "    test_num_correct = 0\n",
    "    eval_loss = 0.\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X = X.as_in_context(ctx)\n",
    "        y = y.as_in_context(ctx)\n",
    "\n",
    "        outputs = net(X)\n",
    "        loss = loss_func(outputs, y)\n",
    "        eval_loss += loss.sum().asscalar()\n",
    "        pred = outputs.argmax(axis=1)\n",
    "        test_num_correct += (pred == y.astype('float32')).sum().asscalar()\n",
    "\n",
    "        pred = pred.as_in_context(cpu())\n",
    "        y = y.astype('float32').as_in_context(cpu())\n",
    "        for p, gt in zip(pred, y):\n",
    "            label_counter[gt] += 1\n",
    "            if p == gt:\n",
    "                correct_counter[gt] += 1\n",
    "\n",
    "    eval_loss /= len(test_dataset)\n",
    "    eval_acc = test_num_correct / len(test_dataset)\n",
    "    eval_acc_avg = (correct_counter / (label_counter+1e-10)).mean().asscalar()\n",
    "    \n",
    "    return eval_loss, eval_acc, eval_acc_avg, time.time()-t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no augmentation\n",
    "train_transformer = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([.5, .5, .5], [.5, .5, .5])\n",
    "])\n",
    "\n",
    "eval_transformer = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([.5, .5, .5], [.5, .5, .5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR100(train=True, transform=lambda data,label:(train_transformer(data),label))\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=tconfig.train_batch_size, shuffle=True, \n",
    "                          num_workers=tconfig.train_num_prefetch_workers, last_batch='discard')\n",
    "test_dataset = CIFAR100(train=False, transform=lambda data,label:(eval_transformer(data), label))\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=tconfig.val_batch_size, shuffle=False, \n",
    "                          num_workers=tconfig.val_num_prefetch_workers, last_batch='keep')\n",
    "\n",
    "tconfig.summary(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_steps = 0\n",
    "good_acc_window = [0.]*tconfig.spotter_window_size\n",
    "estop_loss_window = [0.]*tconfig.patience\n",
    "quantize_input_offline = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': tconfig.learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = Timer()\n",
    "size_per_record = tconfig.train_record_per_steps*tconfig.train_batch_size\n",
    "flag_early_stop = False\n",
    "train_loss = 0.\n",
    "train_num_correct = 0\n",
    "t.start()\n",
    "while global_steps < tconfig.max_steps and not flag_early_stop:\n",
    "    for X, y in train_loader:\n",
    "        # Move data to gpu\n",
    "        X = X.as_in_context(gpu(0))\n",
    "        y = y.as_in_context(gpu(0))\n",
    "        # Forward & Backward\n",
    "        with autograd.record():\n",
    "            outputs = net(X)\n",
    "            loss = loss_func(outputs, y)\n",
    "        net.update_ema()\n",
    "        loss.backward()\n",
    "        trainer.step(tconfig.train_batch_size, ignore_stale_grad=True)   # ignore origin batchnorm paramters\n",
    "        train_loss += loss.sum().asscalar()\n",
    "        pred = outputs.argmax(axis=1)\n",
    "        train_num_correct += (pred == y.astype('float32')).sum().asscalar()\n",
    "        \n",
    "        # Record training info\n",
    "        if global_steps and global_steps % tconfig.train_record_per_steps == 0:\n",
    "            t.stop()\n",
    "            writer.add_scalar('{}/train/Speed'.format(tconfig.tb_main_tag), 1.*tconfig.train_record_per_steps/t.pop(), global_steps)\n",
    "            writer.add_scalars('{}/Loss'.format(tconfig.tb_main_tag), {'train': train_loss*len(test_dataset)/size_per_record}, global_steps)\n",
    "            writer.add_scalars('{}/Acc'.format(tconfig.tb_main_tag), {'train': train_num_correct/size_per_record}, global_steps)\n",
    "            train_loss = 0.\n",
    "            train_num_correct = 0\n",
    "            t.start()\n",
    "        # Evaluate\n",
    "        if global_steps and global_steps % tconfig.val_per_steps == 0:\n",
    "            t.stop()\n",
    "            eval_loss, eval_acc, eval_acc_avg, __ = evaluate(net, mconfig.num_class, test_loader, ctx=gpu(0))\n",
    "            writer.add_scalars('{}/Loss'.format(tconfig.tb_main_tag), {'val': eval_loss}, global_steps)\n",
    "            writer.add_scalars('{}/Acc'.format(tconfig.tb_main_tag), {\n",
    "                'val': eval_acc,\n",
    "                'val_avg': eval_acc_avg\n",
    "            }, global_steps)\n",
    "            \n",
    "            # Spotter & Patience\n",
    "            if quantize_input_offline:\n",
    "                # Spotter\n",
    "                good_acc_window.pop(0)\n",
    "                if global_steps >= tconfig.spotter_starts_at and eval_acc > max(good_acc_window):\n",
    "                    print( \"catch a good model with acc {:.6f} at {} step\".format(eval_acc, global_steps) )\n",
    "                    writer.add_text(tconfig.tb_main_tag, \"catch a good model with acc {:.6f}\".format(eval_acc), global_steps)\n",
    "                    net.save_parameters(\"{}/{}-{:06d}.params\".format(tconfig.checkpoint_dir, tconfig.checkpoint_prefix, global_steps))\n",
    "                good_acc_window.append(eval_acc)\n",
    "                \n",
    "                # Early stop\n",
    "                estop_loss_window.pop(0)\n",
    "                estop_loss_window.append(eval_loss)\n",
    "                if global_steps > tconfig.val_per_steps*len(estop_loss_window):\n",
    "                    min_index = estop_loss_window.index( min(estop_loss_window) )\n",
    "                    writer.add_scalar('{}/val/Patience'.format(tconfig.tb_main_tag), min_index, global_steps)\n",
    "                    if min_index == 0:\n",
    "                        flag_early_stop = True\n",
    "                        print(\"early stop at {} steps\".format(global_steps))\n",
    "                        break\n",
    "            \n",
    "            t.start()\n",
    "        \n",
    "        # Snapshot\n",
    "        if global_steps and global_steps % tconfig.snapshot_per_steps == 0:\n",
    "            net.save_parameters(\"{}/{}-{:06d}.params\".format(tconfig.checkpoint_dir, tconfig.checkpoint_prefix, global_steps))\n",
    "        \n",
    "        # Quantize input offline\n",
    "        if(global_steps == tconfig.quant_offline_after):\n",
    "            quantize_input_offline = True\n",
    "            net.quantize_input_offline()\n",
    "            \n",
    "        # Next step\n",
    "        global_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()\n",
    "exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
